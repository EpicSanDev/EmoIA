# EmoIA - Intelligence Artificielle Ã‰motionnelle ğŸ¤–â¤ï¸

[![Version](https://img.shields.io/badge/version-3.0.0-blue.svg)](https://github.com/emoia/emoia)
[![Python](https://img.shields.io/badge/python-3.11+-green.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-orange.svg)](LICENSE)
[![Docker](https://img.shields.io/badge/docker-ready-blue.svg)](https://www.docker.com/)

EmoIA est une intelligence artificielle avancÃ©e dotÃ©e de capacitÃ©s Ã©motionnelles, conÃ§ue pour crÃ©er des interactions plus naturelles et empathiques entre humains et machines.

## ğŸŒŸ FonctionnalitÃ©s Principales

- **ğŸ­ Analyse Ã‰motionnelle AvancÃ©e** : DÃ©tection et analyse des Ã©motions dans les conversations avec 11 Ã©motions distinctes
- **ğŸ§  Profil de PersonnalitÃ©** : Analyse de personnalitÃ© basÃ©e sur le modÃ¨le Big Five avec extensions Ã©motionnelles
- **ğŸ’¾ MÃ©moire Intelligente** : SystÃ¨me de mÃ©moire hiÃ©rarchique avec consolidation automatique
- **ğŸŒ Multilingue** : Support natif du franÃ§ais, anglais et espagnol avec dÃ©tection automatique
- **ğŸ“Š Analytics en Temps RÃ©el** : Dashboard avec visualisations des Ã©motions et insights
- **ğŸ¤ Suggestions Contextuelles** : GÃ©nÃ©ration de suggestions intelligentes basÃ©es sur le contexte Ã©motionnel
- **ğŸ”„ Apprentissage Continu** : AmÃ©lioration constante basÃ©e sur les interactions
- **ğŸ¦™ Multi-ModÃ¨les IA** : Support de multiples modÃ¨les via MCP (Ollama, OpenAI, etc.)
- **ğŸ¨ Interface UI/UX Moderne** : Design professionnel et responsive

## ğŸ†• Nouvelles FonctionnalitÃ©s v3.0

### ï¿½ Model Context Protocol (MCP)
- Architecture flexible pour intÃ©grer diffÃ©rents modÃ¨les IA
- Changement de modÃ¨le en temps rÃ©el
- Support du streaming pour les rÃ©ponses
- Gestion unifiÃ©e des contextes de conversation

### ğŸ¦™ IntÃ©gration Ollama
- ModÃ¨les IA locaux (Llama2, Mistral, Phi, etc.)
- Pas de dÃ©pendance cloud
- Performance optimisÃ©e avec support GPU
- Installation automatique des modÃ¨les

### ğŸ¨ Interface AmÃ©liorÃ©e
- SÃ©lecteur de modÃ¨les intÃ©grÃ©
- Visualisations temps rÃ©el des Ã©motions
- Design moderne avec thÃ¨me clair/sombre
- Composants React optimisÃ©s

## ï¿½ğŸš€ DÃ©marrage Rapide

### PrÃ©requis

- Docker et Docker Compose
- Python 3.11+ (pour le dÃ©veloppement local)
- Node.js 18+ (pour le frontend)

### Installation avec Docker (RecommandÃ©)

1. **Cloner le repository**
   ```bash
   git clone https://github.com/emoia/emoia.git
   cd emoia
   ```

2. **DÃ©marrer avec le script amÃ©liorÃ©**
   ```bash
   # Rendre le script exÃ©cutable
   chmod +x start_docker_enhanced.sh
   
   # Mode dÃ©veloppement (recommandÃ© pour commencer)
   ./start_docker_enhanced.sh development
   
   # Mode production avec toutes les fonctionnalitÃ©s
   ./start_docker_enhanced.sh production
   
   # Mode avec monitoring (Prometheus + Grafana)
   ./start_docker_enhanced.sh monitoring
   ```

3. **AccÃ©der Ã  l'application**
   - ğŸŒ Frontend: http://localhost:3000
   - ğŸ“¡ API: http://localhost:8000
   - ğŸ“š Documentation API: http://localhost:8000/docs
   - ğŸ¦™ Ollama: http://localhost:11434

### Installation Manuelle

1. **Backend**
   ```bash
   # CrÃ©er un environnement virtuel
   python -m venv venv
   source venv/bin/activate  # Linux/Mac
   # ou
   venv\Scripts\activate  # Windows

   # Installer les dÃ©pendances
   pip install -r requirements.txt

   # DÃ©marrer l'API
   python -m uvicorn src.core.api:app --reload
   ```

2. **Frontend**
   ```bash
   cd frontend
   npm install
   npm start
   ```

## ğŸ“– Guide d'Utilisation

### API REST

#### Chat avec l'IA
```bash
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "user123",
    "message": "Bonjour, comment allez-vous?",
    "preferences": {
      "language": "fr"
    }
  }'
```

#### Chat avec MCP (Multi-ModÃ¨les)
```bash
curl -X POST "http://localhost:8000/mcp/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "user123",
    "message": "Raconte-moi une histoire",
    "provider": "ollama",
    "model": "llama2"
  }'
```

#### Obtenir des suggestions
```bash
curl -X POST "http://localhost:8000/suggestions" \
  -H "Content-Type: application/json" \
  -d '{
    "context": "Je me sens un peu stressÃ© par le travail",
    "emotional_state": {"dominant_emotion": "anxiety"},
    "max_suggestions": 5
  }'
```

### WebSocket pour le temps rÃ©el

```javascript
const ws = new WebSocket('ws://localhost:8000/ws/chat');

ws.onopen = () => {
  // S'identifier
  ws.send(JSON.stringify({
    type: 'identify',
    user_id: 'user123'
  }));

  // Envoyer un message
  ws.send(JSON.stringify({
    type: 'chat_message',
    message: 'Bonjour!',
    user_id: 'user123'
  }));
};

ws.onmessage = (event) => {
  const data = JSON.parse(event.data);
  console.log('RÃ©ponse:', data);
};
```

### WebSocket MCP (Streaming)

```javascript
const ws = new WebSocket('ws://localhost:8000/ws/mcp');

ws.send(JSON.stringify({
  type: 'mcp_stream',
  provider: 'ollama',
  model: 'mistral',
  message: 'Explique-moi la photosynthÃ¨se'
}));
```

## ğŸ—ï¸ Architecture

```
EmoIA/
â”œâ”€â”€ src/                    # Code source principal
â”‚   â”œâ”€â”€ core/              # Logique mÃ©tier principale
â”‚   â”‚   â”œâ”€â”€ api.py         # API FastAPI
â”‚   â”‚   â””â”€â”€ emoia_main.py  # Classe principale EmoIA
â”‚   â”œâ”€â”€ emotional/         # Module d'intelligence Ã©motionnelle
â”‚   â”œâ”€â”€ memory/            # SystÃ¨me de mÃ©moire
â”‚   â”œâ”€â”€ models/            # ModÃ¨les et LLM
â”‚   â””â”€â”€ mcp/               # Model Context Protocol
â”‚       â”œâ”€â”€ mcp_manager.py # Gestionnaire MCP
â”‚       â””â”€â”€ providers/     # Providers de modÃ¨les
â”œâ”€â”€ frontend/              # Application React
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/    # Composants React
â”‚   â”‚   â””â”€â”€ App.tsx        # Composant principal
â”œâ”€â”€ tests/                 # Tests unitaires
â”œâ”€â”€ config.yaml            # Configuration
â””â”€â”€ docker-compose.yml     # Orchestration Docker
```

## ğŸ§ª Tests

```bash
# ExÃ©cuter tous les tests
pytest

# Tests avec couverture
pytest --cov=src tests/

# Tests spÃ©cifiques
pytest tests/test_emoia_main.py
pytest tests/test_mcp.py
```

## ğŸ“Š Analyse Ã‰motionnelle

EmoIA analyse 11 Ã©motions distinctes :
- **Ã‰motions primaires** : joie, tristesse, colÃ¨re, peur, surprise, dÃ©goÃ»t
- **Ã‰motions complexes** : amour, excitation, anxiÃ©tÃ©, contentement, curiositÃ©

Chaque Ã©motion est Ã©valuÃ©e sur une Ã©chelle de 0 Ã  1 avec un score de confiance.

## ğŸ”§ Configuration

Le fichier `config.yaml` permet de personnaliser :
- ModÃ¨les d'IA utilisÃ©s
- ParamÃ¨tres Ã©motionnels
- Configuration de la mÃ©moire
- ParamÃ¨tres d'apprentissage
- Configuration MCP

```yaml
emotional:
  empathy_threshold: 0.7
  emotional_intensity: 0.8
  base_personality:
    openness: 0.8
    conscientiousness: 0.7

mcp:
  default_provider: ollama
  providers:
    ollama:
      base_url: http://ollama:11434
      default_model: llama2
```

## ğŸ¯ Gestion des ModÃ¨les

### Installer de nouveaux modÃ¨les Ollama

```bash
# Lister les modÃ¨les disponibles
docker exec emoia-ollama ollama list

# Installer un modÃ¨le
docker exec emoia-ollama ollama pull llama2:13b
docker exec emoia-ollama ollama pull mistral
docker exec emoia-ollama ollama pull codellama
```

### Changer de modÃ¨le via l'API

```bash
curl -X POST "http://localhost:8000/mcp/switch-model" \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "user123",
    "provider": "ollama",
    "model": "mistral"
  }'
```

## ğŸ“š Documentation ComplÃ¨te

- [Architecture MCP et Ollama](README_MCP_OLLAMA.md)
- [Guide d'installation dÃ©taillÃ©](README_INSTALL.md)
- [Documentation Frontend](README_FRONTEND.md)
- [Guide de migration](MIGRATION_GUIDE.md)

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! Veuillez consulter [CONTRIBUTING.md](CONTRIBUTING.md) pour les directives.

1. Fork le projet
2. CrÃ©er une branche (`git checkout -b feature/AmazingFeature`)
3. Commit vos changements (`git commit -m 'Add AmazingFeature'`)
4. Push sur la branche (`git push origin feature/AmazingFeature`)
5. Ouvrir une Pull Request

## ğŸ“ Licence

Ce projet est sous licence MIT - voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

## ğŸ™ Remerciements

- Ã‰quipe Hugging Face pour les modÃ¨les de transformers
- CommunautÃ© FastAPI pour le framework web
- Ã‰quipe Ollama pour les modÃ¨les locaux
- Contributeurs open source

## ğŸ“ Contact

- Site web : [emoia.ai](https://emoia.ai)
- Email : contact@emoia.ai
- GitHub : [@emoia](https://github.com/emoia)

---

Fait avec â¤ï¸ par l'Ã©quipe EmoIA
