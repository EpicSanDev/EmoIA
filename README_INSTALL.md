# ğŸ¤– EmoIA - Intelligence Artificielle Ã‰motionnelle

<div align="center">
  <img src="https://img.shields.io/badge/Version-3.0.0-blue.svg" alt="Version">
  <img src="https://img.shields.io/badge/Python-3.8+-green.svg" alt="Python">
  <img src="https://img.shields.io/badge/React-18.2+-61DAFB.svg" alt="React">
  <img src="https://img.shields.io/badge/License-MIT-yellow.svg" alt="License">
</div>

## ğŸ“‹ Table des matiÃ¨res

- [Introduction](#introduction)
- [FonctionnalitÃ©s](#fonctionnalitÃ©s)
- [Architecture](#architecture)
- [Installation](#installation)
- [Configuration](#configuration)
- [Utilisation](#utilisation)
- [API Documentation](#api-documentation)
- [DÃ©veloppement](#dÃ©veloppement)
- [DÃ©pannage](#dÃ©pannage)

## ğŸŒŸ Introduction

EmoIA est une intelligence artificielle Ã©motionnelle avancÃ©e qui comprend et s'adapte aux Ã©motions humaines. Elle offre une expÃ©rience conversationnelle empathique et personnalisÃ©e grÃ¢ce Ã  l'analyse Ã©motionnelle en temps rÃ©el et l'apprentissage continu.

### Points forts
- ğŸ­ **Analyse Ã©motionnelle avancÃ©e** : DÃ©tection et comprÃ©hension des Ã©motions
- ğŸ§  **MÃ©moire intelligente** : Apprentissage et mÃ©morisation des interactions
- ğŸŒ **Multilingue** : Support franÃ§ais, anglais et espagnol
- ğŸ¨ **Interface moderne** : Design Ã©purÃ© avec thÃ¨mes clair/sombre
- ğŸ“Š **Analytics en temps rÃ©el** : Tableaux de bord et insights Ã©motionnels
- ğŸ”’ **Respect de la vie privÃ©e** : DonnÃ©es stockÃ©es localement

## ğŸ—ï¸ Architecture

```
EmoIA/
â”œâ”€â”€ frontend/          # Interface React
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ App.tsx
â”‚   â”‚   â””â”€â”€ i18n.ts
â”‚   â””â”€â”€ public/
â”œâ”€â”€ src/              # Backend Python
â”‚   â”œâ”€â”€ core/         # Logique principale
â”‚   â”œâ”€â”€ emotional/    # Analyse Ã©motionnelle
â”‚   â”œâ”€â”€ memory/       # SystÃ¨me de mÃ©moire
â”‚   â”œâ”€â”€ models/       # ModÃ¨les IA
â”‚   â””â”€â”€ analytics/    # WebSocket temps rÃ©el
â”œâ”€â”€ config.yaml       # Configuration
â””â”€â”€ main.py          # Point d'entrÃ©e CLI
```

## ğŸš€ Installation

### PrÃ©requis

- **Python 3.8+**
- **Node.js 16+** et npm
- **Git**
- 8GB RAM minimum
- 10GB d'espace disque (pour les modÃ¨les IA)

### Installation rapide (Linux/macOS)

```bash
# Cloner le projet
git clone https://github.com/votre-repo/EmoIA.git
cd EmoIA

# Lancer l'installation automatique
./start.sh
```

### Installation manuelle

#### 1. Backend Python

```bash
# CrÃ©er un environnement virtuel
python3 -m venv venv
source venv/bin/activate  # Linux/macOS
# ou
venv\Scripts\activate  # Windows

# Installer les dÃ©pendances
pip install -r requirements.txt

# TÃ©lÃ©charger les ressources NLTK
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('vader_lexicon')"
```

#### 2. Frontend React

```bash
cd frontend
npm install
```

#### 3. Configuration Ollama (optionnel)

Pour utiliser Mistral en local :

```bash
# Installer Ollama
curl -fsSL https://ollama.com/install.sh | sh

# TÃ©lÃ©charger Mistral
ollama pull mistral
```

## âš™ï¸ Configuration

### Configuration de base

Ã‰ditez `config.yaml` selon vos besoins :

```yaml
# ModÃ¨le de langage
models:
  language_model: "mistralai/Mistral-7B-Instruct-v0.2"
  language_model_device: "auto"  # "cpu", "cuda", ou "auto"

# ParamÃ¨tres Ã©motionnels
emotional:
  empathy_threshold: 0.7
  emotional_intensity: 0.8

# API
communication:
  api_host: "localhost"
  api_port: 8000
```

### Variables d'environnement (optionnel)

CrÃ©ez un fichier `.env` :

```env
# Pour utiliser OpenAI au lieu de Mistral
OPENAI_API_KEY=your_api_key

# Pour Telegram Bot (optionnel)
TELEGRAM_TOKEN=your_telegram_token
```

## ğŸ“± Utilisation

### DÃ©marrage rapide

```bash
./start.sh
```

Ou manuellement :

```bash
# Terminal 1 - Backend
python -m uvicorn src.core.api:app --host 0.0.0.0 --port 8000

# Terminal 2 - Frontend
cd frontend && npm start
```

### AccÃ¨s aux services

- ğŸŒ **Interface Web** : http://localhost:3000
- ğŸ”§ **API REST** : http://localhost:8000
- ğŸ“š **Documentation API** : http://localhost:8000/docs
- ğŸ”Œ **WebSocket** : ws://localhost:8001/ws/chat

### Interface en ligne de commande

```bash
python main.py
```

Commandes disponibles :
- `help` : Afficher l'aide
- `stats` : Statistiques systÃ¨me
- `insights` : Insights Ã©motionnels
- `quit` : Quitter

## ğŸ“¡ API Documentation

### Endpoints principaux

#### Chat
```http
POST /chat
{
  "user_id": "string",
  "message": "string"
}
```

#### Analytics
```http
GET /analytics/{user_id}
```

#### PrÃ©fÃ©rences
```http
GET /utilisateur/preferences/{user_id}
POST /utilisateur/preferences
```

### WebSocket

```javascript
const ws = new WebSocket('ws://localhost:8001/ws/chat');

ws.send(JSON.stringify({
  user_id: "demo-user",
  message: "Bonjour EmoIA!"
}));
```

## ğŸ› ï¸ DÃ©veloppement

### Structure du code

```python
# Exemple d'utilisation de l'API Python
from src.core import EmoIA
from src.config import Config

async def main():
    config = Config()
    emoia = EmoIA(config)
    await emoia.initialize()
    
    response = await emoia.process_message(
        user_input="Comment vas-tu ?",
        user_id="dev-user"
    )
    print(response)
```

### Ajout de nouvelles fonctionnalitÃ©s

1. **Nouvelles Ã©motions** : Modifier `src/emotional/core.py`
2. **Nouveaux modÃ¨les** : Ajouter dans `src/models/`
3. **Nouvelles langues** : Ã‰diter `frontend/src/i18n.ts`

### Tests

```bash
# Tests backend
pytest tests/

# Tests frontend
cd frontend && npm test
```

## ğŸ”§ DÃ©pannage

### ProblÃ¨mes courants

#### Erreur de mÃ©moire
```bash
# Augmenter la limite de mÃ©moire Python
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
```

#### Port dÃ©jÃ  utilisÃ©
```bash
# Changer le port dans config.yaml
api_port: 8001  # au lieu de 8000
```

#### ModÃ¨les non trouvÃ©s
```bash
# VÃ©rifier que les modÃ¨les sont tÃ©lÃ©chargÃ©s
python -c "from transformers import pipeline; pipeline('sentiment-analysis')"
```

### Logs

Les logs sont disponibles dans :
- `logs/emoia.log` : Logs principaux
- Console : Logs en temps rÃ©el

## ğŸ“ˆ Performance

### Optimisations recommandÃ©es

1. **GPU** : Utiliser CUDA pour de meilleures performances
2. **Cache Redis** : Activer pour les dÃ©ploiements multi-utilisateurs
3. **ModÃ¨les quantifiÃ©s** : RÃ©duire l'utilisation mÃ©moire

### MÃ©triques

- Temps de rÃ©ponse moyen : < 500ms
- Utilisation mÃ©moire : ~4GB (CPU) / ~6GB (GPU)
- Connexions simultanÃ©es : 100+

## ğŸ¤ Contribution

Les contributions sont bienvenues ! Voir [CONTRIBUTING.md](CONTRIBUTING.md)

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir [LICENSE](LICENSE)

## ğŸ™ Remerciements

- Hugging Face pour les modÃ¨les de transformers
- FastAPI pour le framework API
- React pour l'interface utilisateur
- La communautÃ© open source

---

<div align="center">
  Fait avec â¤ï¸ par l'Ã©quipe EmoIA
</div>