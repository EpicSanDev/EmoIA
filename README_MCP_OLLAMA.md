# EmoIA - Documentation MCP et Ollama

## üöÄ Vue d'ensemble

EmoIA int√®gre maintenant le **Model Context Protocol (MCP)** et **Ollama** pour offrir une exp√©rience IA multi-mod√®les flexible et puissante.

## üìã Table des mati√®res

1. [Architecture MCP](#architecture-mcp)
2. [Int√©gration Ollama](#int√©gration-ollama)
3. [Guide de d√©marrage rapide](#guide-de-d√©marrage-rapide)
4. [Configuration des mod√®les](#configuration-des-mod√®les)
5. [API MCP](#api-mcp)
6. [Interface utilisateur](#interface-utilisateur)
7. [D√©veloppement](#d√©veloppement)

## üèóÔ∏è Architecture MCP

Le Model Context Protocol (MCP) permet √† EmoIA d'utiliser diff√©rents mod√®les IA de mani√®re transparente.

### Composants principaux

```
src/mcp/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ mcp_manager.py      # Gestionnaire principal MCP
‚îú‚îÄ‚îÄ mcp_provider.py     # Classe de base pour les providers
‚îú‚îÄ‚îÄ mcp_client.py       # Client simplifi√© pour les interactions
‚îî‚îÄ‚îÄ providers/
    ‚îú‚îÄ‚îÄ ollama_provider.py    # Provider Ollama
    ‚îî‚îÄ‚îÄ openai_provider.py    # Provider OpenAI (√† venir)
```

### Flux de donn√©es

```mermaid
graph LR
    A[Interface Web] --> B[API FastAPI]
    B --> C[MCP Manager]
    C --> D[Provider Ollama]
    C --> E[Provider OpenAI]
    D --> F[Mod√®les Locaux]
    E --> G[API Cloud]
```

## ü¶ô Int√©gration Ollama

Ollama permet d'ex√©cuter des mod√®les IA open-source localement.

### Mod√®les support√©s

- **Llama 2** : Mod√®le de dialogue performant
- **Mistral** : Mod√®le rapide et efficace
- **Phi** : Mod√®le compact pour t√¢ches courantes
- **Gemma** : Mod√®le Google optimis√©
- **Neural-chat** : Sp√©cialis√© dans la conversation

### Installation de nouveaux mod√®les

```bash
# Lister les mod√®les disponibles
docker exec emoia-ollama ollama list

# Installer un nouveau mod√®le
docker exec emoia-ollama ollama pull llama2:13b
docker exec emoia-ollama ollama pull codellama
docker exec emoia-ollama ollama pull mixtral
```

## üöÄ Guide de d√©marrage rapide

### 1. D√©marrage avec Docker

```bash
# Donner les permissions d'ex√©cution
chmod +x start_docker_enhanced.sh

# D√©marrage en mode d√©veloppement
./start_docker_enhanced.sh development

# D√©marrage en mode production avec toutes les fonctionnalit√©s
./start_docker_enhanced.sh production

# D√©marrage avec monitoring
./start_docker_enhanced.sh monitoring
```

### 2. V√©rification des services

```bash
# V√©rifier Ollama
curl http://localhost:11434/api/tags

# V√©rifier les providers MCP
curl http://localhost:8000/mcp/providers

# Lister les mod√®les disponibles
curl http://localhost:8000/mcp/models
```

### 3. Acc√®s √† l'interface

- **Frontend** : http://localhost:3000
- **API Docs** : http://localhost:8000/docs
- **Ollama API** : http://localhost:11434

## ‚öôÔ∏è Configuration des mod√®les

### Configuration par d√©faut

```yaml
# config.yaml
mcp:
  default_provider: ollama
  providers:
    ollama:
      base_url: http://ollama:11434
      default_model: llama2
      timeout: 300
    openai:
      api_key: ${OPENAI_API_KEY}
      default_model: gpt-3.5-turbo
```

### Variables d'environnement

```bash
# .env
OLLAMA_BASE_URL=http://ollama:11434
MCP_DEFAULT_PROVIDER=ollama
OLLAMA_KEEP_ALIVE=24h
```

## üì° API MCP

### Endpoints disponibles

#### Lister les providers
```http
GET /mcp/providers
```

R√©ponse :
```json
{
  "providers": {
    "ollama": {
      "name": "ollama",
      "capabilities": ["text-generation", "chat", "embeddings"],
      "default_model": "llama2",
      "status": "active"
    }
  }
}
```

#### Lister les mod√®les
```http
GET /mcp/models
```

#### Chat avec MCP
```http
POST /mcp/chat
Content-Type: application/json

{
  "user_id": "user123",
  "message": "Bonjour!",
  "provider": "ollama",
  "model": "llama2",
  "temperature": 0.7
}
```

#### WebSocket Streaming
```javascript
const ws = new WebSocket('ws://localhost:8000/ws/mcp');

ws.send(JSON.stringify({
  type: 'mcp_stream',
  provider: 'ollama',
  model: 'llama2',
  message: 'Raconte-moi une histoire'
}));
```

## üé® Interface utilisateur

### Composant ModelSelector

Le nouveau composant `ModelSelector` permet de :
- Visualiser tous les providers disponibles
- S√©lectionner un mod√®le sp√©cifique
- Voir les capacit√©s de chaque provider
- Changer de mod√®le en temps r√©el

### Utilisation dans React

```typescript
import ModelSelector from './components/ModelSelector';

<ModelSelector 
  userId={userId}
  onModelChange={(provider, model) => {
    console.log(`Nouveau mod√®le: ${model} (${provider})`);
  }}
/>
```

## üõ†Ô∏è D√©veloppement

### Ajouter un nouveau provider

1. Cr√©er une classe h√©ritant de `MCPProvider` :

```python
from src.mcp.mcp_provider import MCPProvider

class MyProvider(MCPProvider):
    async def _setup(self):
        # Configuration du provider
        pass
    
    async def send_completion(self, model, messages, **kwargs):
        # Logique de compl√©tion
        pass
    
    async def list_models(self):
        # Retourner la liste des mod√®les
        pass
```

2. Enregistrer le provider :

```python
# Dans mcp_manager.py
from .providers.my_provider import MyProvider

async def _load_default_providers(self):
    my_provider = MyProvider()
    await self.register_provider("myprovider", my_provider)
```

### Tests

```bash
# Tests unitaires MCP
pytest tests/test_mcp.py

# Tests d'int√©gration
pytest tests/test_mcp_integration.py
```

## üîç Monitoring et debugging

### Logs Ollama

```bash
# Voir les logs Ollama
docker logs emoia-ollama

# Suivre les logs en temps r√©el
docker logs -f emoia-ollama
```

### M√©triques de performance

Avec le mode monitoring activ√© :
- **Prometheus** : http://localhost:9090
- **Grafana** : http://localhost:3001

Dashboards disponibles :
- Utilisation des mod√®les
- Temps de r√©ponse par provider
- Taux d'erreur
- Utilisation m√©moire/CPU

## üö® Troubleshooting

### Ollama ne d√©marre pas

```bash
# V√©rifier l'√©tat
docker ps -a | grep ollama

# Red√©marrer le service
docker-compose restart ollama

# V√©rifier les logs
docker logs emoia-ollama
```

### Mod√®les non disponibles

```bash
# Lister les mod√®les install√©s
docker exec emoia-ollama ollama list

# Installer manuellement un mod√®le
docker exec -it emoia-ollama ollama pull llama2
```

### Performance GPU

Pour utiliser le GPU avec Ollama :

```bash
# V√©rifier la disponibilit√© GPU
docker run --rm --gpus all nvidia/cuda:11.5.0-base-ubuntu20.04 nvidia-smi

# D√©marrer avec support GPU
docker-compose -f docker-compose.gpu.yml up
```

## üìö Ressources

- [Documentation Ollama](https://ollama.ai/docs)
- [Model Context Protocol Spec](https://github.com/anthropics/model-context-protocol)
- [FastAPI WebSocket Guide](https://fastapi.tiangolo.com/advanced/websockets/)
- [React TypeScript Best Practices](https://react-typescript-cheatsheet.netlify.app/)

## ü§ù Contribution

Pour contribuer au d√©veloppement MCP/Ollama :

1. Fork le repository
2. Cr√©er une branche feature (`git checkout -b feature/nouveau-provider`)
3. Commiter vos changements
4. Push et cr√©er une PR

---

Pour toute question ou probl√®me, ouvrez une issue sur GitHub ou contactez l'√©quipe de d√©veloppement.